import OpenAI from "openai";
import { get_encoding } from "tiktoken";

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// ‚úÖ DECLARADO FUERA PARA USARLO ANTES
const systemPrompt = `
Sos AltIA, el AGENTE DE IA de EtherCode, una empresa argentina que crea soluciones digitales personalizadas. Tu tarea es responder consultas sobre:

- ü§ñ AGENTES AUTOM√ÅTICOS de IA (para WhatsApp, web, redes sociales)
- üß† Automatizaci√≥n de procesos (conexi√≥n de APIs, CRMs, n8n)
- üåê Desarrollo Web (Next.js, TypeScript, PostgreSQL)

Respond√© con claridad y buena onda, solo sobre temas de EtherCode. No respondas sobre pol√≠tica, religi√≥n, OpenAI, ni temas t√©cnicos no relacionados.

Si el usuario quiere contratar o hablar con un humano, derivalo con este mensaje:

> ¬°Genial! üôå Para avanzar con la contrataci√≥n, pod√©s escribirle a nuestro asesor humano en WhatsApp:  
> üëâ https://wa.me/5493884486112?text=Hola%2C%20el%20bot%20me%20deriv%C3%B3%20y%20quiero%20comunicarme%20con%20un%20humano.

Si no sab√©s algo, dec√≠ que un asesor humano lo resolver√°.
`.trim();

export default async function handler(req, res) {
  if (req.method === "POST") {
    const { message } = req.body;

    if (!message) {
      return res.status(400).json({ error: "Mensaje es requerido" });
    }

    const encoding = get_encoding("cl100k_base");

    // ‚úÖ calculamos tokens de prompt + message
    const totalInput = `${systemPrompt}\n${message}`;
    const tokenCount = encoding.encode(totalInput).length;
    console.log("cantidad de tokens usados para envia", tokenCount);

    // L√≠mite seguro para GPT-3.5 (m√°x 16.385 tokens)
    const maxTokensForResponse = 500;

    if (tokenCount + maxTokensForResponse > 16000) {
      return res.status(400).json({
        error: "El mensaje es demasiado largo. Reduce el n√∫mero de caracteres.",
      });
    }

    encoding.free();

    try {
      const chatCompletion = await openai.chat.completions.create({
        messages: [
          { role: "system", content: systemPrompt },
          { role: "user", content: message },
        ],
        model: "gpt-3.5-turbo",
        max_tokens: maxTokensForResponse,
        temperature: 0.7,
      });

      const reply = chatCompletion.choices[0].message.content;
      return res.status(200).json({ reply });
    } catch (error) {
      console.error("Error al llamar a OpenAI:", error);
      return res.status(500).json({ error: "Error interno del servidor" });
    }
  } else {
    return res.status(405).json({ error: "M√©todo no permitido" });
  }
}
